        % % Title and author(s)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Accelerating Joins with Filters}
\author{Nicholas Corrado \and Xiating Ouyang}
\date{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[10pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %
% % The next command allows your in import encapsulated
% % postscript files, .epsf or .eps files, which
% % contain vector graphic image data.
% %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx}
\usepackage{charter,eulervm}
\usepackage{simpleConference}
%\renewcommand{\baselinestretch}{1.5}
\setcounter{secnumdepth}{3} % default value for 'report' class is "2"
\usepackage{amsthm,amsmath,amssymb,upgreek,marvosym,mathtools}
\usepackage{array}
\usepackage{makeidx}  % allows for indexgeneration
\usepackage{paralist}
\usepackage{subfig}
\usepackage{tabularx}
\usepackage{tabu}
\usepackage{comment}
\usepackage[nottoc]{tocbibind}
\usepackage[usenames,dvipsnames]{color}
\usepackage[pdftex,breaklinks,colorlinks,citecolor={blue}, linkcolor={blue},urlcolor=Maroon]{hyperref}
\usepackage{tkz-graph}
 \linespread{1.25}
\usetikzlibrary{automata, positioning,arrows,shapes,decorations.pathmorphing}

 \tikzset{
->, % makes the edges directed
>=stealth, % makes the arrow heads bold
node distance=3cm, % specifies the minimum distance between two nodes. Change if necessary.
every state/.style={thick, fill=gray!10}, % sets the properties for each ’state’ node
initial text=$ $, % sets the text that appears on the start arrow
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{reduction}{Reduction}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{scolium}{Scolium}[section]   %% And a not so common one.
\newtheorem{definition}{Definition}[section]
\newtheorem{conjecture}{Conjecture}[section]
\newtheorem{corollary}{Corollary}[section]
%\newenvironment{proof}{{\sc Proof:}}{~\hfill QED}
\newenvironment{AMS}{}{}
\newenvironment{keywords}{}{}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\newcommand{\todo}{(TO BE CONTINUED...)}

\newcommand{\paris}[1]{{\color{blue} Paris: [{#1}]}}

\newcommand{\xiating}[1]{{\color{blue} Xiating: [{#1}]}}


\newcommand{\trans}[1]{
	#1^\mathsf{T}
}

\newcommand{\db}{$\mathbf{db}$}
\newcommand{\sjfq}{\texttt{sjfCQA}}
\newcommand{\bcq}{\texttt{bcq}}
\newcommand{\problem}[1]{\textsc{certainty}($#1$)}
\newcommand{\FO}{$\mathbf{FO}$}
\newcommand{\PTIME}{$\mathbf{P}$}
\newcommand{\LSPACE}{$\mathbf{L}$}
\newcommand{\coNP}{$\mathbf{coNP}$}
\newcommand{\und}[1]{\underline{#1}}
\newcommand{\NL}{$\mathbf{NL}$}
\newcommand{\JOIN}{\bowtie}

\begin{document}
\newpage
\maketitle


\abstract{In query optimization on star schemas, lookahead information passing (LIP) is a strategy exploiting the efficiency of probing succinct filters to eliminate practically all facts that do not appear in the final join results before performing the actual join. Assuming data independency across all columns in the fact table, LIP achieves efficient and robust query optimization. We present some variants of LIP that can achieve empirically efficient query execution on fact table with correlated and even adversarial data columns, experimented on a skeleton database on top of Apache Arrow. We also analyze the performance of each variant of LIP using the notion of competitive ratio in online algorithms.}


\section{Introduction}

Performing join operations in database management systems using Star Schemas is a fundamental and prevalent task in the modern data industry. Continuous efforts have been spent on building a reliable query optimizer over the last few decades. However, the current optimizers may still produce disastrously inefficient query plans which involves processing unnecessarily gigantic intermediate tables \cite{leis2015good,rabl2013variations}. The \textit{lookahead Information Passing (LIP)} strategy aggressively uses Bloom Filters to filter the fact tables to effectively reduce the sizes of the intermediate tables, provably as efficient and robust as computing the join using the optimal query plan \cite{zhu2017looking}. The key idea behind LIP is to estimate the filter selectivity of each dimension table and adaptively reorder the sequence of applying the filters to the fact table. 


The LIP strategy can also be modeled using an online algorithm setting. Suppose we fix $n$ filters, and the tuples in the fact table arrives in an online fashion. Upon arrival of each tuple, one has to decide a sequence of filters to probe the tuple, with an objective of minimizing the number of probes needed to decide whether to accept the tuple and forward it to the hash join phase, or to eliminate it. A mechanism deciding the sequence of applying the filters is thus crucial to the success of LIP. If a tuple passes all filters, \textit{all} mechanisms have to probe the tuple to all filters to confirm its passage; and if a tuple is eliminated by the filters, the \textit{optimal} mechanism would apply any filter that rejects the tuple in the first place, using only one probe. Thus given any fact table $F$, the number of probes that an optimal mechanism requires to process all tuples in the fact table can be readily computed: 
\[
	\textsc{OPT}(F) = n|F_{\text{pass}}| + |F_{\text{reject}}|,
\]
where $|F_{\text{pass}}|$ and $|F_{\text{reject}}|$ are the number of tuples in $F$ that pass all filters and are rejected in $F$ respectively. For any mechanism $\mathcal{M}$, denoted by \textsc{ALG}$_{\mathcal{M}}(F)$ the number of probes that $\mathcal{M}$ performed to process all tuples in the fact table.  The performance of any mechanism $\mathcal{M}$ can thus be measured by 
\[
	\max_{F}\frac{\textsc{ALG}_{\mathcal{M}}(F)}{\textsc{OPT}(F)},
\]
called the \textit{competitive ratio} of $\mathcal{M}$. The competitive ratio is always at least 1 by definition, and in this problem the competitive ratio is at most $n$, the number of filters, since one mechanism can probe each tuple to at most $n$ filters.


In the practical perspective however, one wishes to minimize the total running time of LIP, which is effectively the sum of the running time of the mechanism and the running time of building the filters and performing the probes. A trade-off between having a near optimal mechanism that consumes much time and allowing many failed probes to eliminate each non-participating tuple is therefore of much interest. This project tries to balance the two levels by designing a good mechanism 


This project aims at designing efficient LIP mechanisms and measure their performance in terms of their competitive ratio and their overall running time. We will build a skeleton database system on top of Apache Arrow supporting LIP and hash-joins to conduct our experiments and test the performance of our LIP mechanisms against the hash-join.


%Experimental results show that as LIP uses more filters, the increase in performance improvement diminishes, displaying a concave curve. One reason is that if the cache cannot hold all filters, then evicting the filters causes significant overhead in the performance. Moreover, when probing each fact tuple against all filters, the more selective filters are inserted to cache first. When the cache is full, the cache manager has to evict a more selective filter so that a less selective filter can be inserted to cache. Once LIP switches to probing the next fact tuple, the more selective filter is inserted into the cache again, while one could have skipped probing against certain inselective filters to reduce the replacement overhead.


%This project aims at investigating the effect of skipping certain LIP filter on improving the performance of LIP, and if possible, derive a theoretical guarantee on the performance of LIP against the optimal joining sequence. We will build a skeleton database system on top of Apache Arrow supporting LIP and hash-joins to conduct our experiments and test the performance of our revised LIP strategy against the hash-join.


\section{Lookahead Information Passing (LIP)}

The LIP strategy has three stages: (1) Building a hash table and a filter for each dimension table, (2) probe each fact tuple on the filters, producing a set of fact tuples with false positives, and (3) probe the hash table of each dimension table to eliminate the false positives. In what follows we mainly discuss stage (2) since stages (1) and (3) are readily implemented by either the database engine or the filter constructors.

Let $F$ be the fact table and $D_i$ the dimension tables for $1 \leq i \leq n$. We denote the number of facts in $F$ and each $D_i$ as $|F|$ and $|D_i|$. A LIP filter on $D_i$ is implemented using a Bloom filter, with false positive rate $\varepsilon$. The true selectivity $\sigma_i$ of $D_i$ on fact table $F$ is given by $\sigma_i = |D_i \JOIN_{pk_i = fk_i} F| / |F|,$ where $pk_i$ is the primary key of $D_i$ and $fk_i$ is the foreign key of $D_i$ in $F$. The \texttt{LIP-join} algorithm, depicted in Figure \ref{fig:lip}, computes the indices of tuples in $F$ that pass the filtering of each Bloom filter of $D_i$. Note that there is an innate false positive rate $\varepsilon$ associated with each Bloom Filter, and thus the set of indices is a superset of the true set of indices of tuples appearing in the final join result.

The partition in \cite{zhu2017looking} satisfies that $|F_{t+1}| = 2|F_{t}|$ at line 5, and the algorithm approximates the true selectiveness $\sigma_i$ of each dimension $D_i$ using $pass[i]/count[i]$, the aggregated selectiveness since the beginning.

\begin{figure*}[h!]
	\centering
	\tikz\path (0,0) node[draw, text width=.8\textwidth, rectangle, inner xsep=20pt, inner ysep=10pt]{
		\begin{minipage}[t!]{\textwidth}
			{\sc Procedure}: \texttt{LIP-join}
			\\
			{\sc Input}: a fact table $F$ and a set of $n$ Bloom filters $f_i$ for each $D_i$ with $1 \leq i \leq n$
 			\\
			{\sc Output}: Indices of tuples in $F$ that pass the filtering
			\begin{tabbing}
				Aaa\=aaA\=Aaa\=Aaa\=Aaa\=AAAAAAAAAAAAAAAAAAAAAAAAA\=A \kill
				1.\> Initialize $I = \emptyset$
				\\
				2.\> {\bf foreach } filter $f$ {\bf do}
				\\
				3.\>\> $count[f] \leftarrow 0$
				\\
				4.\>\> $pass[f] \leftarrow 0$ 
				\\
				5.\> Partition $F = \bigcup_{1 \leq t \leq T}F_t$. 
				\\
				6.\> {\bf foreach } fact block $F_t$ {\bf do} 
				\\
				7.\>\> {\bf foreach } filter $f$ in order {\bf do}
				\\
				8.\>\>\> {\bf foreach} index $j \in F_t$ {\bf do}
				\\
				9.\>\>\>\> $count[f] \leftarrow count[f] + 1$
				\\
				10.\>\>\>\> {\bf if }$f$ contains $F_t[j]$ 
				\\
				11.\>\>\>\>\> $I \leftarrow I \cup \{j\}$ 
				\\
				12.\>\>\>\>\> $pass[f] \leftarrow pass[f] + 1$
				\\
				13.\>\> {\bf sort} filters $f$ in nondesending order of $pass[f]/count[f]$
				\\
				14.\> {\bf return } $I$
			\end{tabbing}  
		\end{minipage}
	};
	\caption{The LIP algorithm for computing the joins.}
	\label{fig:lip}
\end{figure*}


\section{Variant LIP Mechanisms}

In this section, we introduce 3 variants of LIP: LIP-CUT-OFF, LIP-CUT-OFF-RESUR and LIP-K. \xiating{Fill this.}


\subsection{LIP with Cut-Off}
The current LIP strategy builds the hash table on each dimension table in stage (1) and probes each fact tuple to each hash table to eliminate false positives in stage (2). It is somewhat equivalent to performing a hash join of (i) the filtered fact table with false positives and (ii) all the other dimension tables. The slight difference is that the hash join algorithm would inherently choose a joining sequence to perform the join, whereas the LIP strategy may probe each fact tuple to eliminate false positives in any order that the programmer wishes. There is no query optimizer in our constructed system, and thus no optimization overhead if we choose to use hash join to help us probing.

\begin{figure*}[h!]
	\centering
	\tikz\path (0,0) node[draw, text width=.8\textwidth, rectangle, inner xsep=20pt, inner ysep=10pt]{
		\begin{minipage}[t!]{\textwidth}
			{\sc Procedure}: \texttt{LIP-Cut-Off}
			\\
			{\sc Input}: a fact table $F$ and a set of $n$ Bloom filters $f_i$ for each $D_i$ with $1 \leq i \leq n$
 			\\
			{\sc Output}: Indices of tuples in $F$ that pass the filtering
			\begin{tabbing}
				Aaa\=aaA\=Aaa\=Aaa\=Aaa\=AAAAAAAAAAAAAAAAAAAAAAAAA\=A \kill
				1.\> Initialize $I = \emptyset$
				\\
				2.\> {\bf foreach } filter $f$ {\bf do}
				\\
				3.\>\> $count[f] \leftarrow 0$
				\\
				4.\>\> $pass[f] \leftarrow 0$ 
				\\
				5.\> Partition $F = \bigcup_{1 \leq t \leq T}F_t$. 
				\\
				6.\> {\bf foreach } fact block $F_t$ {\bf do} 
				\\
				7.\>\> {\bf foreach } filter $f$ in order {\bf do}
				\\
				8.\>\>\> {\bf foreach} index $j \in F_t$ {\bf do}
				\\
				9.\>\>\>\> $count[f] \leftarrow count[f] + 1$
				\\
				10.\>\>\>\> {\bf if }$f$ contains $F_t[j]$ 
				\\
				11.\>\>\>\>\> $I \leftarrow I \cup \{j\}$ 
				\\
				12.\>\>\>\>\> $pass[f] \leftarrow pass[f] + 1$
				\\
				13.\>\> {\bf sort} filters $f$ in nondesending order of $pass[f]/count[f]$
				\\
				14.\> {\bf return } $I$
			\end{tabbing}  
		\end{minipage}
	};
	\caption{The LIP algorithm for computing the joins.}
	\label{fig:lip-cut}
\end{figure*}

\subsection{LIP with Cut-Off and Resurrection}

\begin{figure*}[h!]
	\centering
	\tikz\path (0,0) node[draw, text width=.8\textwidth, rectangle, inner xsep=20pt, inner ysep=10pt]{
		\begin{minipage}[t!]{\textwidth}
			{\sc Procedure}: \texttt{LIP-Cut-Off-with-Resurrection}
			\\
			{\sc Input}: a fact table $F$ and a set of $n$ Bloom filters $f_i$ for each $D_i$ with $1 \leq i \leq n$
 			\\
			{\sc Output}: Indices of tuples in $F$ that pass the filtering
			\begin{tabbing}
				Aaa\=aaA\=Aaa\=Aaa\=Aaa\=AAAAAAAAAAAAAAAAAAAAAAAAA\=A \kill
				1.\> Initialize $I = \emptyset$
				\\
				2.\> {\bf foreach } filter $f$ {\bf do}
				\\
				3.\>\> $count[f] \leftarrow 0$
				\\
				4.\>\> $pass[f] \leftarrow 0$ 
				\\
				5.\> Partition $F = \bigcup_{1 \leq t \leq T}F_t$. 
				\\
				6.\> {\bf foreach } fact block $F_t$ {\bf do} 
				\\
				7.\>\> {\bf foreach } filter $f$ in order {\bf do}
				\\
				8.\>\>\> {\bf foreach} index $j \in F_t$ {\bf do}
				\\
				9.\>\>\>\> $count[f] \leftarrow count[f] + 1$
				\\
				10.\>\>\>\> {\bf if }$f$ contains $F_t[j]$ 
				\\
				11.\>\>\>\>\> $I \leftarrow I \cup \{j\}$ 
				\\
				12.\>\>\>\>\> $pass[f] \leftarrow pass[f] + 1$
				\\
				13.\>\> {\bf sort} filters $f$ in nondesending order of $pass[f]/count[f]$
				\\
				14.\> {\bf return } $I$
			\end{tabbing}  
		\end{minipage}
	};
	\caption{The LIP algorithm for computing the joins.}
	\label{fig:lip-resur}
\end{figure*}

\subsection{LIP with $k$ memory}


LIP algorithm in Figure \ref{fig:lip} estimates the selectivity of each filter using statistics from the very beginning, which is inefficient for certain distribution and physical layout of data. Consider some filter $f$ that is very selective for the first $t_0$ iterations at line 6 and not selective for the remaining iterations. (For example, a filter $f$ filtering for \texttt{year} $\geq 2017$ and the \texttt{Date} table is sorted in \texttt{year}.) In this case, LIP would obtain a good estimate of the selectivity of $f$ during the first $t_0$ iterations, and thus tend to apply $f$ early in the remaining iterations. However, it is in fact more efficient to postpone applying $f$ in the remaining iterations, despite $f$ has good selectivity in the first $t_0$ iterations. One remedy to this is to only ``remember" the hit/miss statistics of each filter over the previous $t^*$ iterations.

\begin{figure*}[h!]
	\centering
	\tikz\path (0,0) node[draw, text width=.8\textwidth, rectangle, inner xsep=20pt, inner ysep=10pt]{
		\begin{minipage}[t!]{\textwidth}
			{\sc Procedure}: \texttt{LIP-$k$}
			\\
			{\sc Input}: a fact table $F$ and a set of $n$ Bloom filters $f_i$ for each $D_i$ with $1 \leq i \leq n$
 			\\
			{\sc Output}: Indices of tuples in $F$ that pass the filtering
			\begin{tabbing}
				Aaa\=aaA\=Aaa\=Aaa\=Aaa\=AAAAAAAAAAAAAAAAAAAAAAAAA\=A \kill
				1.\> Initialize $I = \emptyset$
				\\
				2.\> {\bf foreach } filter $f$ {\bf do}
				\\
				3.\>\> Initialize $count[f] \leftarrow 0$, $pass[f] \leftarrow 0$ 
				\\
				4.\>\> Initialize $count\_queue[f]$ with $k$ zeros and $pass\_queue[f]$ with $k$ zeros.
				\\
				5.\> Partition $F = \bigcup_{1 \leq t \leq T}F_t$. 
				\\
				6.\> {\bf foreach } fact block $F_t$ {\bf do} 
				\\
				7.\>\> {\bf foreach } filter $f$ in order {\bf do}
				\\
				8.\>\>\> {\bf foreach} index $j \in F_t$ {\bf do}
				\\
				9.\>\>\>\> $count[f] \leftarrow count[f] + 1$
				\\
				10.\>\>\>\> {\bf if }$f$ contains $F_t[j]$ 
				\\
				11.\>\>\>\>\> $I \leftarrow I \cup \{j\}$ 
				\\
				12.\>\>\>\>\> $pass[f] \leftarrow pass[f] + 1$
				\\
				13.\>\>\> Dequeue one element from both $count\_queue[f]$ and $pass\_queue[f]$
				\\
				14.\>\>\> Enqueue $count[f]$ and $pass[f]$ to $count\_queue[f]$ and $pass\_queue[f]$ respectively
				\\
				15.\>\>\> Reset $count[f] \leftarrow 0$, $pass[f] \leftarrow 0$ 
				\\
				16.\>\> {\bf sort} filters $f$ in nondesending order of $sum(pass\_queue[f])/sum(count\_queue[f])$
				\\
				17.\> {\bf return } $I$
			\end{tabbing}  
		\end{minipage}
	};
	\caption{The LIP algorithm for computing the joins.}
	\label{fig:lip-k}
\end{figure*}



\section{Database Implementation}

We have developed a prototype database system supporting basic join/select operations on star schemas sufficient to benchmark the performance of LIP and its variants on top of Apache Arrow. We assume that the fact table schema contains foreignkeys to all dimension tables, and each dimension table is single-key. Given a star schema fact table $F$ and dimension tables $D_i$ for $1 \leq i \leq n$, a join query in our system specifies selectors $\sigma_F$ for $F$ and $\sigma_i$ for each $D_i$, and executing that query will return $\sigma_F(F) \JOIN \sigma_1(D_1) \JOIN \dots \JOIN \sigma_n(D_n)$, projected on the schema of $F$. 

The supported premitive selectors are comparison with integer/string values ($=, \leq, \geq, <, >$) and between, where the semantic of \texttt{BETWEE} ($\ell$, $h$) is to select all $x$ with $\ell \leq x \leq h$. The selectors for each dimension can be either a premitive selector, or a composition (logical AND and OR) of multiple primitive/composite selectors. This is implemented using the Composite design pattern.


The hash join algorithm first produces a hash table $T_i$ for each $\sigma_i(D_i)$, projected on the $k_i$, and then probe each tuple in the fact table against all $T_i$. We used Sparseepp (accessible at \url{https://github.com/greg7mdp/sparsepp}) as our implementation of the hash table, in which the sparsehash by Google (accessible at \url{https://github.com/sparsehash/sparsehash}) is used as the underlying hash function. All primary keys are regarded as 64-bit integers.

The succinct filter structure we choose is the Bloom filters. The default false-positive rate is set to 0.001 and the default number of inserts to the filter is set to 50,000. The hash function for the Bloom Filter is Knuth's Multiplicative hash function, extended to accept a 64-bit integer as a seed. 


Our code is available at \url{https://github.com/NicholasCorrado/CS764}.


\section{Empirical Results}


The dataset for testing is obtained from the Star Schema Benchmark \cite{o2009star}. We will hard-code each queries in \cite{o2009star} to measure the join processing time.






% \section{Skipping LIP Filters}

% In this section, we present our modified \texttt{LIP-join-skip} algorithm and discuss some implementation details.

% \subsection{Implementation}

% We implemented the traditional hash-join algorithm, the \texttt{LIP-join} algorithm and our modified \texttt{LIP-joip-skip} on top of apache arrow in C++. The code is available at \url{https://github.com/NicholasCorrado/CS764}.


% \section{Experiments}


\begin{comment}
\section{Deploying lookahead filters in distributed systems}

In this section we discuss a method to deploy the LIP filters in distributed systems. This method incorporates LIP and the \textsc{hypercube} algorithm \cite{zhu2017looking,}.


Let $p$ be the number of machines available. Let $k_i$ be the primary key of $D_i$, and each tuple in the fact table $F$ possesses a foreign key to each $D_i$. Suppose $p = \prod_{1 \leq i \leq n} p_i$,  and then we label each of the $p$ machines with a coordinate $(x_1, x_2, \dots, x_n)$ where each $1 \leq x_i \leq p_i$.

We first pick $n$ hash functions $h_i$ such that the range of each $h_i$ is $\{1, 2, \dots, p_i\}$. Then for each dimension table $D_i$ and for each primary key $k_i$ in $D_i$, we send $k_i$ to all machines with $i$-th component being $h_i(k_i)$. 

\end{comment}


% \section{Concluding remarks}

\section{Future works}

In this work, all variants have an adversarial fact table that would force the algorithm to achieve a competitive ratio of $n$. However, reasonable applications of some existing online algorithm mechanisms may yield a competitive ratio of $O(\log n)$ using the weighted majority algorithm \cite{littlestone1994weighted}.


\section*{Acknowledgement}
The authors wish to thank Prof.\ Jignesh Patel for constant feedbacks on this project and Kevin Gaffney for helping us with Apache Arrow specifics.


\bibliography{rep}{}
\bibliographystyle{plain}

\end{document}
