        % % Title and author(s)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Topics in Database Management Systems Project Report: Skipping Filters is Almost Optimal}
\author{Nicholas Corrado \and Xiating Ouyang}
\date{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[10pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %
% % The next command allows your in import encapsulated
% % postscript files, .epsf or .eps files, which
% % contain vector graphic image data.
% %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx}
%\usepackage{charter,eulervm}
\usepackage{simpleConference}
%\renewcommand{\baselinestretch}{1.5}
\setcounter{secnumdepth}{3} % default value for 'report' class is "2"
\usepackage{amsthm,amsmath,amssymb,upgreek,marvosym,mathtools}
\usepackage{array}
\usepackage{makeidx}  % allows for indexgeneration
\usepackage{paralist}
\usepackage{subfig}
\usepackage{tabularx}
\usepackage{tabu}
\usepackage{comment}
\usepackage[nottoc]{tocbibind}
\usepackage[usenames,dvipsnames]{color}
\usepackage[pdftex,breaklinks,colorlinks,citecolor={blue}, linkcolor={blue},urlcolor=Maroon]{hyperref}
\usepackage{tkz-graph}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=25mm,
 top=20mm,
 right=25mm,
 }
\usetikzlibrary{automata, positioning,arrows,shapes,decorations.pathmorphing}

 \tikzset{
->, % makes the edges directed
>=stealth, % makes the arrow heads bold
node distance=3cm, % specifies the minimum distance between two nodes. Change if necessary.
every state/.style={thick, fill=gray!10}, % sets the properties for each ’state’ node
initial text=$ $, % sets the text that appears on the start arrow
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{reduction}{Reduction}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{scolium}{Scolium}[section]   %% And a not so common one.
\newtheorem{definition}{Definition}[section]
\newtheorem{conjecture}{Conjecture}[section]
\newtheorem{corollary}{Corollary}[section]
%\newenvironment{proof}{{\sc Proof:}}{~\hfill QED}
\newenvironment{AMS}{}{}
\newenvironment{keywords}{}{}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\newcommand{\todo}{(TO BE CONTINUED...)}

\newcommand{\paris}[1]{{\color{blue} Paris: [{#1}]}}


\newcommand{\trans}[1]{
	#1^\mathsf{T}
}

\newcommand{\db}{$\mathbf{db}$}
\newcommand{\sjfq}{\texttt{sjfCQA}}
\newcommand{\bcq}{\texttt{bcq}}
\newcommand{\problem}[1]{\textsc{certainty}($#1$)}
\newcommand{\FO}{$\mathbf{FO}$}
\newcommand{\PTIME}{$\mathbf{P}$}
\newcommand{\LSPACE}{$\mathbf{L}$}
\newcommand{\coNP}{$\mathbf{coNP}$}
\newcommand{\und}[1]{\underline{#1}}
\newcommand{\NL}{$\mathbf{NL}$}
\newcommand{\JOIN}{\bowtie}

\begin{document}
\newpage
\maketitle


%\abstract{}


\section{Introduction}

Performing join operations in database management systems using Star Schemas is a fundamental and prevalent task in the industry. Various efforts have been spent on building a reliable query optimizer. However, the current optimizers may still produce disastrously inefficient query plans which involves processing unnecessarily gigantic intermediate tables \cite{leis2015good,rabl2013variations}. \textit{Lookahead Information Passing (LIP)} aggressively uses Bloom Filters to filter the fact tables to effectively reduce the sizes of the intermediate tables, provably as efficient and robust as computing the join using the optimal query plan \cite{zhu2017looking}. The key procedure is to estimate the filter selectivity of each dimension table, and adaptively reorder the sequence of applying the filters to the fact table.  

Experimental results show that as LIP uses more filters, the increase in performance improvement diminishes, displaying a concave curve. One reason is that if the cache cannot hold all filters, then evicting the filters causes significant overhead in the performance. Moreover, when probing each fact tuple against all filters, the more selective filters are inserted to cache first. Once the cache is full, the cache manager has to evict a more selective filter so that a less selective filter can be inserted to cache. Once LIP switches to probing the next fact tuple, the more selective filter is inserted into the cache again, while one could have skipped probing against certain inselective filters to reduce the replacement overhead.



This project aims at investigating the effect of skipping certain LIP filter on improving the performance of LIP, and if possible, derive a theoretical guarantee on the performance of LIP against the optimal joining sequence. 



\section{Lookahead Information Passing (LIP)}

The LIP procedure has three stages: (1) Building a hash table and a filter for each dimension table, (2) probe each fact tuple on the filters, producing a set of fact tuples with false positives, and (3) perform hash-join to eliminate the false positives. In what follows we mainly discuss stage (2) since stages (1) and (3) are readily implemented by either the database engine or the filter constructors.

Let $F$ be the fact table and $D_i$ the dimension tables for $1 \leq i \leq n$. We denote the number of facts in $F$ and each $D_i$ as $|F|$ and $|D_i|$. A LIP filter on $D_i$ is implemented using a Bloom filter, with false positive rate $\varepsilon$. The true selectivity $\sigma_i$ of $D_i$ on fact table $F$ is given by $\sigma_i = |D_i \JOIN_{pk_i = fk_i} F| / |F|,$ where $pk_i$ is the primary key of $D_i$ and $fk_i$ is the foreign key of $D_i$ in $F$. The \texttt{LIP-join} algorithm, depicted in Figure \ref{fig:lip}, computes the indices of tuples in $F$ that pass the filtering of each Bloom filter of $D_i$. Note that there is an innate false positive rate $\varepsilon$ associated with each Bloom Filter, and thus the set of indices is a superset of the true set of indices of tuples appearing in the final join result.

The partition in \cite{zhu2017looking} satisfies that $|F_{t+1}| = 2|F_{t}|$ at line 5, and the algorithm approximates the true selectiveness $\sigma_i$ of each dimension $D_i$ using $pass[i]/count[i]$, the aggregated selectiveness since the beginning.

\begin{figure*}[h!]
	\centering
	\tikz\path (0,0) node[draw, text width=.8\textwidth, rectangle, inner xsep=20pt, inner ysep=10pt]{
		\begin{minipage}[t!]{\textwidth}
			{\sc Procedure}: \texttt{LIP-join}
			\\
			{\sc Input}: a fact table $F$ and a set of $n$ Bloom filters $f_i$ for each $D_i$ with $1 \leq i \leq n$
 			\\
			{\sc Output}: Indices of tuples in $F$ that pass the filtering
			\begin{tabbing}
				Aaa\=aaA\=Aaa\=Aaa\=Aaa\=AAAAAAAAAAAAAAAAAAAAAAAAA\=A \kill
				1.\> Initialize $I = \emptyset$
				\\
				2.\> {\bf foreach } filter $f$ {\bf do}
				\\
				3.\>\> $count[f] \leftarrow 0$
				\\
				4.\>\> $pass[f] \leftarrow 0$ 
				\\
				5.\> Partition $F = \bigcup_{1 \leq t \leq T}F_t$. 
				\\
				6.\> {\bf foreach } fact block $F_t$ {\bf do} 
				\\
				7.\>\> {\bf foreach } filter $f$ in order {\bf do}
				\\
				8.\>\>\> {\bf foreach} index $j \in F_t$ {\bf do}
				\\
				9.\>\>\>\> $count[f] \leftarrow count[f] + 1$
				\\
				10.\>\>\>\> {\bf if }$f$ contains $F_t[j]$ 
				\\
				11.\>\>\>\>\> $I \leftarrow I \cup \{j\}$ 
				\\
				12.\>\>\>\>\> $pass[f] \leftarrow pass[f] + 1$
				\\
				13.\>\> {\bf sort} filters $f$ in nondesending order of $pass[f]/count[f]$
				\\
				14.\> {\bf return } $I$
			\end{tabbing}  
		\end{minipage}
	};
	\caption{The LIP algorithm for computing the joins.}
	\label{fig:lip}
\end{figure*}


\subsection{Discussions}

LIP algorithm in Figure \ref{fig:lip} estimates the selectivity of each filter using statistics from the very beginning, which is inefficient for certain distribution and physical layout of data. Consider some filter $f$ that is very selective for the first $t_0$ iterations at line 6 and not selective for the remaining iterations. (For example, a filter $f$ filtering for \texttt{year} $\geq 2017$ and the \texttt{Date} table is sorted in \texttt{year}.) In this case, LIP would obtain a good estimate of the selectivity of $f$ during the first $t_0$ iterations, and thus tend to apply $f$ early in the remaining iterations. However, it is in fact more efficient to postpone applying $f$ in the remaining iterations, despite $f$ has good selectivity in the first $t_0$ iterations. One remedy to this is to only ``remember" the hit/miss statistics of each filter over the previous $t^*$ iterations.


\section{Our Work}

Our work can be divided into the following milestones.

\begin{enumerate}
	\item Implement a system supporting hash-join on top of apache arrow;
	\item Implement LIP processing in our system; and
	\item Test multiple variants of LIP and measure their performance.
\end{enumerate}


Note that the system we are building is intended to be a \textit{minimal} prototype of a database system on top of apache arrow supporting only selection and join as the basic operations using hash-join and LIP-join to model any database system. 

The dataset for testing is obtained from the Star Schema Benchmark \cite{o2009star}. We will hard-code each queries in \cite{o2009star} to measure the join processing time.

\section{Current Progress}

We have finished milestone 1 and our system is implemented in C++, on top of apache arrow. Our implementation for selection currently only supports comparing integers and strings. For strings we only support $=$, and for integers we only support $=, <, >, \leq, \geq$ and \texttt{between}, where the semantics for \texttt{between}($\ell, h$) is selecting all integers $x$ with $\ell \leq x \leq h$. 

More figures will be produced for sure!

Our code is available at \url{https://github.com/NicholasCorrado/CS764}. 



% \section{Skipping LIP Filters}

% In this section, we present our modified \texttt{LIP-join-skip} algorithm and discuss some implementation details.

% \subsection{Implementation}

% We implemented the traditional hash-join algorithm, the \texttt{LIP-join} algorithm and our modified \texttt{LIP-joip-skip} on top of apache arrow in C++. The code is available at \url{https://github.com/NicholasCorrado/CS764}.


% \section{Experiments}


\begin{comment}
\section{Deploying lookahead filters in distributed systems}

In this section we discuss a method to deploy the LIP filters in distributed systems. This method incorporates LIP and the \textsc{hypercube} algorithm \cite{zhu2017looking,}.


Let $p$ be the number of machines available. Let $k_i$ be the primary key of $D_i$, and each tuple in the fact table $F$ possesses a foreign key to each $D_i$. Suppose $p = \prod_{1 \leq i \leq n} p_i$,  and then we label each of the $p$ machines with a coordinate $(x_1, x_2, \dots, x_n)$ where each $1 \leq x_i \leq p_i$.

We first pick $n$ hash functions $h_i$ such that the range of each $h_i$ is $\{1, 2, \dots, p_i\}$. Then for each dimension table $D_i$ and for each primary key $k_i$ in $D_i$, we send $k_i$ to all machines with $i$-th component being $h_i(k_i)$. 

\end{comment}


% \section{Concluding remarks}

\section{Future works}

We have already implemented the Bloom Filter for milestone 2 and the LIP algorithm is.

We will first try to run experiments to produce the concave curve of running time against the number of Bloom Filters used, and then empirically experiment the ``optimal" number of filters to be used in stage (2) of the LIP algorithm.

A revised version of LIP that only remembers the past $t^*$ probing statistics will be implemented for experimental purpose. 



\section*{Acknowledgement}
The authors wish to thank Prof.\ Jignesh Patel for constant feedbacks on this project and Kevin Gaffney for helping us with apache arrow.


\bibliography{rep}{}
\bibliographystyle{plain}

\end{document}
