
\section{Lookahead Information Passing (LIP)}
In this section, we first present the LIP strategy in \cite{zhu2017looking}. We then discuss our variant LIP-$k$, designed to respond to local skewness more quickly than LIP. Finally, we discuss the competitive ratios of all deterministic mechanisms and provide proof on its lower bound. Possible extensions of LIP using randomness is also discussed.

\subsection{LIP}
The LIP strategy has three stages: (1) Build a hash table and a filter for each dimension table, (2) probe each fact tuple on the filters, producing a set of fact tuples with false positives, and (3) probe the hash table of each dimension table to eliminate the false positives. In what follows, we mainly discuss stage (2), since stages (1) and (3) are readily implemented by the filter constructors and database engine, respectively.

Let $F$ be the fact table and $D_i$ the dimension tables for $1 \leq i \leq n$. We denote the number of facts in $F$ and each $D_i$ as $|F|$ and $|D_i|$. Each LIP filter on $D_i$ is a Bloom filter with false positive rate $\varepsilon$. The true selectivity $\sigma_i$ of $D_i$ on fact table $F$ is given by $\sigma_i = |D_i \JOIN_{pk_i = fk_i} F| / |F|,$ where $pk_i$ is the primary key of $D_i$ and $fk_i$ is the foreign key of $D_i$ in $F$. The \texttt{LIP-join} algorithm, depicted in Figure \ref{fig:lip}, computes the indices of tuples in $F$ that pass all LIP filters. Note that because of the false positive rate $\varepsilon$ associated with each filter, the set of indices is a superset of the true set of indices of tuples appearing in the final join result.

The partition in \cite{zhu2017looking} satisfies that $|F_{t+1}| = 2|F_{t}|$ at line 5, and the algorithm approximates the true selectiveness $\sigma_i$ of each dimension $D_i$ using $pass[i]/count[i]$, the aggregated selectiveness since the beginning.

\begin{figure*}[h!]
	\centering
	\tikz\path (0,0) node[draw, text width=.8\textwidth, rectangle, inner xsep=20pt, inner ysep=10pt]{
		\begin{minipage}[t!]{\textwidth}
			{\sc Procedure}: \texttt{LIP-join}
			\\
			{\sc Input}: a fact table $F$ and a set of $n$ Bloom filters $f_i$ for each $D_i$ with $1 \leq i \leq n$
 			\\
			{\sc Output}: Indices of tuples in $F$ that pass the filtering
			\begin{tabbing}
				Aaa\=aaA\=Aaa\=Aaa\=Aaa\=AAAAAAAAAAAAAAAAAAAAAAAAA\=A \kill
				1.\> Initialize $I = \emptyset$
				\\
				2.\> {\bf foreach } filter $f$ {\bf do}
				\\
				3.\>\> $count[f] \leftarrow 0$
				\\
				4.\>\> $pass[f] \leftarrow 0$ 
				\\
				5.\> Partition $F = \bigcup_{1 \leq t \leq T}F_t$. 
				\\
				6.\> {\bf foreach } fact block $F_t$ {\bf do} 
				\\
				7.\>\> {\bf foreach } filter $f$ in order {\bf do}
				\\
				8.\>\>\> {\bf foreach} index $j \in F_t$ {\bf do}
				\\
				9.\>\>\>\> $count[f] \leftarrow count[f] + 1$
				\\
				10.\>\>\>\> {\bf if }$f$ contains $F_t[j]$ 
				\\
				11.\>\>\>\>\> $I \leftarrow I \cup \{j\}$ 
				\\
				12.\>\>\>\>\> $pass[f] \leftarrow pass[f] + 1$
				\\
				13.\>\> {\bf sort} filters $f$ in nondesending order of $pass[f]/count[f]$
				\\
				14.\> {\bf return } $I$
			\end{tabbing}  
		\end{minipage}
	};
	\caption{The LIP algorithm for computing the joins.}
	\label{fig:lip}
\end{figure*}


\subsection{LIP-$k$}

The LIP strategy in Figure \ref{fig:lip} estimates the selectivity of each filter using statistics from all previous batches, 
which can be inefficient for certain foreign key distributions in the fact table. 
Consider the case where some filter $f$ is very selective for the first $T$ batches and not selective for the remaining batches. 
(For example, a filter $f$ filtering for \texttt{year} $\geq 2017$ and the \texttt{Date} table is sorted in \texttt{year}.) In this case, LIP would obtain a good estimate of the selectivity of $f$ during the first $T$ iterations, and thus tend to apply $f$ early in the remaining iterations. However, it is more efficient to postpone applying $f$ in the remaining iterations, despite $f$ has good selectivity in the first $T$ iterations. One remedy to this is to only ``remember" the hit/miss statistics of each filter over the previous $k$ batches.

\begin{figure*}[h!]
	\centering
	\tikz\path (0,0) node[draw, text width=.8\textwidth, rectangle, inner xsep=20pt, inner ysep=10pt]{
		\begin{minipage}[t!]{\textwidth}
			{\sc Procedure}: \texttt{LIP-$k$}
			\\
			{\sc Input}: a fact table $F$ and a set of $n$ Bloom filters $f_i$ for each $D_i$ with $1 \leq i \leq n$
 			\\
			{\sc Output}: Indices of tuples in $F$ that pass the filtering
			\begin{tabbing}
				Aaa\=aaA\=Aaa\=Aaa\=Aaa\=AAAAAAAAAAAAAAAAAAAAAAAAA\=A \kill
				1.\> Initialize $I = \emptyset$
				\\
				2.\> {\bf foreach } filter $f$ {\bf do}
				\\
				3.\>\> Initialize $count[f] \leftarrow 0$, $pass[f] \leftarrow 0$ 
				\\
				4.\>\> Initialize $count\_queue[f]$ with $k$ zeros and $pass\_queue[f]$ with $k$ zeros.
				\\
				5.\> Partition $F = \bigcup_{1 \leq t \leq T}F_t$. 
				\\
				6.\> {\bf foreach } fact block $F_t$ {\bf do} 
				\\
				7.\>\> {\bf foreach } filter $f$ in order {\bf do}
				\\
				8.\>\>\> {\bf foreach} index $j \in F_t$ {\bf do}
				\\
				9.\>\>\>\> $count[f] \leftarrow count[f] + 1$
				\\
				10.\>\>\>\> {\bf if }$f$ contains $F_t[j]$ 
				\\
				11.\>\>\>\>\> $I \leftarrow I \cup \{j\}$ 
				\\
				12.\>\>\>\>\> $pass[f] \leftarrow pass[f] + 1$
				\\
				13.\>\>\> $count\_queue[f].dequeue()$ and $pass\_queue[f].dequeue()$
				\\
				14.\>\>\> $count\_queue[f].enqueue(count[f])$ and $pass\_queue[f].enqueue(pass[f])$
				\\
				15.\>\>\> Reset $count[f] \leftarrow 0$, $pass[f] \leftarrow 0$ 
				\\
				16.\>\> {\bf sort} filters $f$ in nondesending order of $sum(pass\_queue[f])/sum(count\_queue[f])$
				\\
				17.\> {\bf return } $I$
			\end{tabbing}  
		\end{minipage}
	};
	\caption{The LIP algorithm for computing the joins.}
	\label{fig:lip-k}
\end{figure*}

Empirical data shows that for the orignal SSB dataset and certain queries, LIP-$k$ is as fast as LIP, and for certain datasets and queries LIP-$k$ is faster than LIP. Detailed empirical data are presented and analyzed in Section \ref{sec:experiment}.



\subsection{Competitive Ratio Analysis}

The LIP strategy and its variant LIP-$k$ depicted in Figures \ref{fig:lip} and \ref{fig:lip-k} are \textit{deterministic}, i.e.\ multiple executions over the same fact table would produce the same result. Experimental results show that LIP has faster execution time compared to hash join in the optimal sequence \cite{zhu2017looking} on the benchmark dataset, in which the keys are distributed almost uniformly. However, it can be shown that any deterministic mechanism in the worst case can never achieve a competitive ratio less than $n$ when played against an adversary producing an adversarial dataset.


\begin{theorem}\label{thm:det-n}
	Let $n$ be the number of filters in the LIP problem. There is no deterministic mechanism $\mathcal{M}$ achieving a competitive ratio less than $n$ for the \textsc{LIP} problem.

\end{theorem}

\begin{proof}
	We present an adversary to the arbitrary mechanism $\mathcal{M}$ such that $\mathcal{M}$ only achieves a competitive ratio of $n$ in the worst case. 
	Let the $n$ filters be $f_1, f_2, \dots, f_n$, 
	and let $S_k$ denote the filter sequence that will be used to filter batch $k$. 
	Let each batch contain $m$ tuples.
	At each iteration $k$, prior to LIP's probe phase, the adversary observes $S_k$ and produces a batch of tuples $\{t_1, t_2, \dots, t_m\}$, where $t_i \in f_n$ but $t_i \notin f_j$ for any $j < n$. The adversary then feeds this batch into mechanism $\mathcal{M}$. Thus, $\mathcal{M}$ performs $n$ filter probes to eliminate each tuple, whereas the optimal sequence is to apply $f_n$ first. Thus, $\mathcal{M}$ achieves a competitive ratio of $n$.
\end{proof}

It might be possible to design a randomized mechanism that can achieve a better competitive ratio than $n$. The randomized mechanism would, at the end of each batch, select a sequence of applying the filters from a distribution of all filter permutations, based on the estimated selectivities. However, we have not obtained any algorithmic upper bound on the competitive ratio. Furthermore, a randomized approach creates a new concern of being too computationally heavy.

In the practical perspective however, one wishes to minimize the total running time of the mechanism, which is effectively the sum of the running time of the mechanism and the running time of building the filters and performing the probes. A trade-off between having a near optimal mechanism that consumes much time and allowing many failed probes to eliminate each non-participating tuple is therefore of much interest. 

