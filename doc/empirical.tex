
\section{Empirical Results}\label{sec:experiment}

In this section we present the empirical results obtained from running Hash-join, LIP and LIP-$k$ on several datasets. In Section \ref{sec:dataset}, we present the dataset we used and how we manually apply skewness to generate skewed and adversarial datasets; in Section \ref{sec:time} we present the running time of multiple strategies and discuss their performance; and in Section \ref{sec:ratio} we discuss how $k$ affects the competitive ratio of LIP-$k$ empirically on our dataset.



\subsection{Skewed and Adversarial Datasets}
\label{sec:dataset}
The dataset for testing is obtained from the Star Schema Benchmark \cite{o2009star}. We will hard-code each queries in \cite{o2009star} to measure the join processing time.



\subsection{Execution Time}
\label{sec:time}



\subsection{Competitive Ratio}
\label{sec:ratio}

We ran LIP-$k$ on each skewed dataset we produced and compute the competitive ratio of each LIP-$k$ by taking the maximum of the competitive ratios achieved across all queries in all datasets. The results are depicted in Figure \ref{fig:cr}. 

\begin{figure}
    \centering
  	\subfloat[]{
    	\includegraphics[width=0.43\textwidth,keepaspectratio]{cr-k-uniform}
  	}	
    \quad
    \subfloat[]{
    	\includegraphics[height=0.32\textwidth,keepaspectratio]{cr-k-skewed}
    }
    \caption{The competitive ratios of LIP-$k$ against different $k$ values. We ran LIP-$k$ on uniform data and skewed (and adversarial) dataset to produce (a) and (b) respectively. The data point at $k = 200$ represents LIP (which is essentially LIP-$\infty$).}
    \label{fig:cr}
\end{figure}

When the keys in the fact table columns are distributed uniformly, the filters need not to react to the local changes. Hence for LIP with higher $k$, it remembers more batches in the uniform data, therefore may produce relatively more accurate estimate of the selectivities than the LIP with lower $k$. Hence the competitive ratio would decrease (slightly) when $k$ increases, as depicted in Figure \ref{fig:cr}.  


%@TODO: Explain the adversarial case using epsilons
Figure \ref{fig:cr} (b) displays how an adversarial dataset can make LIP-$k$ and LIP perform inefficiently. In fact, each LIP-$k$ with even $k$ achieves an approximation ratio less than 2; LIP and LIP-$k$ with odd $k$ achieve an approximation ratio of almost 2 precisely at Query 3.2 in dataset \texttt{date-part-adversary}. Query 3.2 has two joins, and thus the performance of LIP-$k$ with even $k$ matches the worst case competitive ratio. By construction of \texttt{date-part-adversary}, the first batch has $\sigma^{1}_{1} = 2/5$ and $\sigma^{1}_{2} = 3/5$, and  each batch $B_{2i+1}$ is an \textit{odd} batch, with $\sigma^{2i}_{1} = 0$ and $\sigma^{2i}_{2} = 1$; and each batch $B_{2i}$ is an \textit{even} batch, with $\sigma^{2i}_{1} = 1$ and $\sigma^{2i}_{2} = 0$. 

For LIP-$k$ with odd $k$, it always contains one more odd (or even) batch than the other, and thus the estimated selectivity and the filtering sequence are in favor of the majority batch type, resulting in a false prediction in the next batch, yielding a competitive ratio of 2. For LIP-$k$ with even $k$, at even sized batches, the estimated selectivities remain the same ($1/2$ by construction) throughout the execution, and thus the sequence of applying filter is never changed, once determined after the first batch. Thus for half batches it is optimal, and for the other half it is worse, resulting a competitive ratio of \[ \frac{1 \times 1/2 + 2 \times 1/2}{1} = 1.5,\] as depicted in Figure \ref{fig:cr}. For LIP, since it remembers the statistics from the beginning, by construction it would also make the same decision as LIP-$k$ with odd $k$, losing at every batch. 
%@TODO: I will explain this better!!!!
\footnote{An astute observer might notice that for $k = 200$, the worst-case competitive ratio of $2$ is achieved even though $k$ is not odd. This is explained by recognizing that LIP-200 is essentially an approximation of LIP. LIP-200 performs just as poorly as LIP up until  the $201^{st}$ batch, after which it estimates both selectivities at 1/2.}







%$k = 1, 2, 3, 4, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, 100$ on 