\section{Database Implementation}

   \section{Empirical Results}\label{sec:experiment}

In this section we present the empirical results obtained from running Hash-join, LIP and LIP-$k$ on several datasets. In Section \ref{sec:dataset}, we present the dataset we used and how we manually apply skewness to generate skewed and adversarial datasets; in Section \ref{sec:time} we present the running time of multiple strategies and discuss their performance; and in Section \ref{sec:ratio} we discuss how $k$ affects the competitive ratio of LIP-$k$ empirically on our dataset.



\subsection{Skewed and Adversarial Datasets}
\label{sec:dataset}
The dataset for testing is obtained from the Star Schema Benchmark \cite{o2009star}. We will hard-code each queries in \cite{o2009star} to measure the join processing time.

We choose to skew the \texttt{ORDER DATE} foreign key in the \texttt{LINEORDER} table for simplicity in generating skew datasets. We skew on the predicate $\texttt{ORDER DATE} = 1997 \vee 1998$.
We generated several skew

Let $f_i^A$ denote the selectivity of a Bloom filter on key A after processing the $i^{th}$ batch. 
Let $\sigma_i^A$ denote the selectivity of a Bloom filter on key A on the $i^{th}$ batch alone. 

\subsubsection{\texttt{lineorder-date-50-50.tbl}}

Skewed datasets are generated by first generating the SSB dataset with $\text{SF} = 1$.
We then edit the columns we want to skew. 

The aim of the adversarial dataset is to force LIP to perform the maximum number of filter probes possible.
We now show how to construct such a dataset where $n = 2$.
Let subscripts denote the batch index and superscripts denote the key column (A or B).
We start with the first batch having
$\sigma_1^A = \frac{1}{2} - \varepsilon$ and $\sigma_1^B = \frac{1}{2} + \varepsilon$ where $0 < \varepsilon < \frac{1}{2}$. Then for all $j > 1$, we let

\begin{align*}
\sigma_j^A &= 
    \begin{cases}
    1 & \text{$j$ even} \\[0.5em]
    0 & \text{$j$ odd} \\
    \end{cases}\\[0.5em]
\sigma_j^B &= 
    \begin{cases}
    0 & \text{$j$ even} \\[0.5em]
    1 &  \text{$j$ odd} \\
    \end{cases}
\end{align*}

Thus, the optimal filter sequence $S^{OPT}$ is 

\begin{align*}
S^{OPT} &= 
    \begin{cases}
    (B, A) & \text{$j$ even} \\[0.5em]
    (A, B) & \text{$j$ odd} \\
    \end{cases}\\[0.5em]
\end{align*}

\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
After processing batch $j$,  we have $f_j^A = \frac{\frac{1}{2} - \varepsilon + \floor{\frac{j}{2}}}{j}$ and $f_j^B = \frac{\frac{1}{2} + \varepsilon + \floor{\frac{j}{2}}}{j}$  which can be rewritten as

\begin{align*}
f_j^A &= 
    \begin{cases}
    \frac{1}{2} + \frac{\frac{1}{2}-\varepsilon}{j} & \text{$j$ even} \\[0.5em]
    \frac{1}{2} - \frac{\varepsilon}{j} &  \text{$j$ odd} \\
    \end{cases}\\[0.5em]
f_j^B &= 
    \begin{cases}
    \frac{1}{2} - \frac{\frac{1}{2}-\varepsilon}{j} & \text{$j$ even} \\[0.5em]
    \frac{1}{2} + \frac{\varepsilon}{j} &  \text{$j$ odd} \\
    \end{cases}\\[0.5em]
\end{align*}

and thus, LIPs filter ordering will be

\begin{align*}
S &= 
    \begin{cases}
    (A, B) & \text{$j$ even} \\[0.5em]
    (B, A) & \text{$j$ odd} \\
    \end{cases}\\[0.5em]
\end{align*}

which is the reverse of $S^{OPT}$. 
Hence, after the first batch has been processed, LIP will have worst-case performance on all remaining batches.


\begin{center}
\begin{tabular}{ |>{\ttfamily}r|>{\ttfamily}c|l| } 
\hline
{\bf Dataset Name} & {\bf Skewed Foreign Keys} & {\bf Description of Skew} \\
\hline
\hline
lineorder-date-50-50.tbl& ORDER DATE & First 50 batches have $\sigma_{{\texttt{ORDER DATE}} = 1997 \text{ OR } 1998} = 1$ \\
& & Next 50 batches have $\sigma_{{\texttt{ORDER DATE}} = 1997 \text{ OR } 1998} = 0$ \\ 
\hline
lineorder-date-first-half.tbl& ORDER DATE & First $N/2$ batches have $\sigma_{{\texttt{ORDER DATE}} = 1997 \text{ OR } 1998} = 1$ \\
& & Next $N/2$ batches have $\sigma_{{\texttt{ORDER DATE}} = 1997 \text{ OR } 1998} = 0$, \\
& & where $N$ is the total number of batches in \text{LINEORDER}\\
\hline
lineorder-date-linear.tbl& ORDER DATE & $\sigma_{{\texttt{ORDER DATE}} = 1997 \text{ OR } 1998}$ increases linearly from 0 to 1 \\
& & across the \texttt{LINEORDER} table\\
\hline
lineorder-date-part-adversary.tbl& ORDER DATE & See Section~\ref{}\\
& &First batch has $\sigma_{{\texttt{ORDER DATE}} = 1997 \text{ OR } 1998} = \frac{1}{2} - \varepsilon$ \\
& & and $\sigma_{\texttt{PART KEY}} = \frac{1}{2} + \varepsilon$ \\
& PART KEY & Next 5 batches have $\sigma_{{\texttt{ORDER DATE}} = 1997 \text{ OR } 1998} = 0$ \\ 
\hline
\end{tabular}
\end{center}

The adversarial dataset is constructed to make SSB query 4.2 achieve near worst-case performance for LIP. 



\subsection{Execution Time}
\label{sec:time}



\subsection{Competitive Ratio}
\label{sec:ratio}

We ran LIP-$k$ on each skewed dataset we produced and compute the competitive ratio of each LIP-$k$ by taking the maximum of the competitive ratios achieved across all queries in all datasets. The results are depicted in Figure \ref{fig:cr}. 

\begin{figure}
    \centering
    \subfloat[]{
        \includegraphics[width=0.43\textwidth,keepaspectratio]{cr-k-uniform}
    }   
    \quad
    \subfloat[]{
        \includegraphics[height=0.32\textwidth,keepaspectratio]{cr-k-skewed}
    }
    \caption{The competitive ratios of LIP-$k$ against different $k$ values. We ran LIP-$k$ on uniform data and skewed (and adversarial) dataset to produce (a) and (b) respectively. The data point at $k = 200$ represents LIP (which is essentially LIP-$\infty$).}
    \label{fig:cr}
\end{figure}

When the keys in the fact table columns are distributed uniformly, the filters need not react to the local changes. Hence for LIP with higher $k$, it remembers more batches in the uniform data, therefore may produce relatively more accurate estimate of the selectivities than the LIP with lower $k$. Hence the competitive ratio would decrease (slightly) when $k$ increases, as depicted in Figure \ref{fig:cr}.  

%@TODO: Explain the adversarial case using epsilons
Figure \ref{fig:cr} (b) displays how an adversarial dataset can make LIP-$k$ and LIP perform inefficiently. In fact, each LIP-$k$ with even $k$ achieves an approximation ratio less than 2; LIP and LIP-$k$ with odd $k$ achieve an approximation ratio of almost 2 precisely at Query 3.2 in dataset \texttt{date-part-adversary}. Query 3.2 has two joins, and thus the performance of LIP-$k$ with even $k$ matches the worst case competitive ratio. By construction of \texttt{date-part-adversary}, the first batch has $\sigma^{1}_{1} = 1/2-\
\varepsilon$ and $\sigma^{1}_{2} = 1/2+\varepsilon$, and  each batch $B_{2i+1}$ is an \textit{odd} batch, with $\sigma^{2i}_{1} = 0$ and $\sigma^{2i}_{2} = 1$; and each batch $B_{2i}$ is an \textit{even} batch, with $\sigma^{2i}_{1} = 1$ and $\sigma^{2i}_{2} = 0$. 

When $i \leq k$, LIP and LIP-$k$ execute identically, since LIP-$k$ has not yet 'forgotten' any previous batches. We now consider the case where $i > k$, {\it i.e.} where LIP-$k$ has forgotten at least the first batch. For odd $k$, LIP-$k$'s selectivity estimates always contains one more odd (or even) batch than the other, and thus the estimated selectivity and filtering sequence are in favor of the majority batch type. This yields a false prediction of the next batch, yielding a competitive ratio of 2. For even $k$, LIP-$k$'s selectivity estimates contain an equal amount of even an odd batches,  and thus estimated selectivities remain the same ($1/2$ by construction) throughout the execution, and the filter sequence does not change. Thus for half batches it is optimal, and for the other half it is worse, resulting in a competitive ratio of \[ \frac{1 \times 1/2 + 2 \times 1/2}{1} = 1.5,\] as depicted in Figure \ref{fig:cr}. For LIP, since it remembers the statistics from the beginning, by construction it would also make the same decision as LIP-$k$ with odd $k$, losing at every batch. 

LIP performs poorly because it never forgets the first batch.
%@TODO: I will explain this better!!!!
\footnote{An astute observer might notice that for $k = 200$, the worst-case competitive ratio of $2$ is achieved even though $k$ is not odd. This is explained by recognizing that LIP-200 is essentially an approximation of LIP. LIP-200 performs just as poorly as LIP up until  the $201^{st}$ batch, after which it estimates both selectivities as 1/2.}



\begin{align*}
\varepsilon_i = 
    \begin{cases}
    \frac{\varepsilon}{2k+1} & \text{for odd $i$, where $i = 2k + 1$} \\ 
    \frac{\frac{1}{2} - \varepsilon}{2k} & \text{for even $i$, where $i = 2k$}
    \end{cases}
\end{align*}

If $k$ is even, then LIP-k will (after the first $k$ batches have been processed) have $\sigma^A_i = \sigma^B_i = \frac{1}{2}$. Thus,

%$k = 1, 2, 3, 4, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, 100$ on 